library(tidyverse)
library(plotly)
library(widgetframe)
library(tidytext)
# source("process_starbucks_data.R")
df <- read_csv("author_sentiment.csv", show_col_types=FALSE)
View(df)
knitr::opts_chunk$set(eval = TRUE, include  = TRUE, warning=FALSE, message=FALSE)
sb_nutr <- read_csv("starbucks-menu-nutrition.csv", show_col_types=FALSE)
View(sb_nutr)
ggplotly(p3)
p3<- df |>
unnest_tokens(word, text_cleaned, token="words") |>
count(word, sort=T) |>
head(20) |>
ggplot(aes(x=word, y=n))+
geom_col()+
coord_flip()
ggplotly(p3)
pl <- ggplot(df, aes(x=TRUE_SENTIMENT, y=avg_len_token, color=TRUE_SENTIMENT)) +
geom_point(alpha=0.8)
ggplotly(pl)
pl <- ggplot(df, aes(x=TRUE_SENTIMENT, y=avg_len_token, color=TRUE_SENTIMENT, size=avg_len_sen)) +
geom_point(alpha=0.8)
ggplotly(pl)
p3<- df |>
unnest_tokens(word, text_cleaned, token="words") |>
count(word, sort=T) |>
head(20) |>
ggplot(aes(x=word, y=n))+
geom_col()+
coord_flip() +
ggtitle("Top 20 common words in the document") +
xlab("Frequency of words") +
ylab("Words")
ggplotly(p3)
pl <- ggplot(df, aes(x=TRUE_SENTIMENT, y=avg_len_token, color=TRUE_SENTIMENT)) +
geom_point(alpha=0.8) +
ggtitle("Scatter plot of average length of token by each sentiment class") +
ylab("Average Length of Token") +
xlab("Sentiment Class")
ggplotly(pl)
pl2 <- ggplot(df, aes(x=TRUE_SENTIMENT, y=num_proper_noun, color=TRUE_SENTIMENT)) +
geom_point(alpha=0.8) +
ggtitle("Scatter plot of number of proper nouns by each sentiment class") +
ylab("Number of Proper Nouns") +
xlab("Sentiment Class")
ggplotly(pl2)
p3<- df |>
unnest_tokens(word, text_cleaned, token="words") |>
summarise(word_frequency=n()) |>
arrange(across(word_frequency, desc)) |>
filter(!(word %in% stopwords("english"))) |>
filter(!(grepl("[[:digit:]]+", word))) |>
head(20) |>
ggplot(aes(reorder(word, -word_frequency), word_frequency)) +
geom_col()+
coord_flip() +
ggtitle("Top 20 common words in the document") +
ylab("Frequency of words") +
xlab("Words")
library(tidyverse)
library(plotly)
library(widgetframe)
library(tidytext)
library(wordcloud)
library(tm)
library(topicmodels)
library(NLP)
library(RColorBrewer)
p3<- df |>
unnest_tokens(word, text_cleaned, token="words") |>
summarise(word_frequency=n()) |>
arrange(across(word_frequency, desc)) |>
filter(!(word %in% stopwords("english"))) |>
filter(!(grepl("[[:digit:]]+", word))) |>
head(20) |>
ggplot(aes(reorder(word, -word_frequency), word_frequency)) +
geom_col()+
coord_flip() +
ggtitle("Top 20 common words in the document") +
ylab("Frequency of words") +
xlab("Words")
p3<- df |>
unnest_tokens(word, text_cleaned, token="words") |>
summarise(word_frequency=n()) |>
arrange(across(word_frequency, desc))
View(p3)
p3<- df |>
select(text_cleaned) |>
unnest_tokens(word, text_cleaned, token="words") |>
summarise(word_frequency=n()) |>
arrange(across(word_frequency, desc))
p3<- df |>
select(text_cleaned) |>
unnest_tokens(word, text_cleaned, token="words") |>
group_by(word) |>
summarise(word_frequency=n()) |>
arrange(across(word_frequency, desc)) |>
filter(!(word %in% stopwords("english"))) |>
filter(!(grepl("[[:digit:]]+", word))) |>
head(20) |>
ggplot(aes(reorder(word, -word_frequency), word_frequency)) +
geom_col()+
coord_flip() +
ggtitle("Top 20 common words in the document") +
ylab("Frequency of words") +
xlab("Words")
ggplotly(p3)
# Customize stopwords
stopwords2 <-c(stopwords("english"), "also", "using", "use", "used", "s", "t")
p3<- df |>
select(text_cleaned) |>
unnest_tokens(word, text_cleaned, token="words") |>
group_by(word) |>
summarise(word_frequency=n()) |>
arrange(across(word_frequency, desc)) |>
filter(!(word %in% stopwords2)) |>
filter(!(grepl("[[:digit:]]+", word))) |>
head(20) |>
ggplot(aes(reorder(word, -word_frequency), word_frequency)) +
geom_col()+
coord_flip() +
ggtitle("Top 20 common words in the document") +
ylab("Frequency of words") +
xlab("Words")
ggplotly(p3)
pl3 <- ggplot(df, aes(x=TRUE_SENTIMENT, y=num_past_verb, color=TRUE_SENTIMENT)) +
geom_point(alpha=0.8) +
ggtitle("Scatter plot of number of past tense verbs by each sentiment class") +
ylab("Number of Past Tense Verbs") +
xlab("Sentiment Class")
ggplotly(pl3)
pl4 <- ggplot(df, aes(x=TRUE_SENTIMENT, y=num_comma, color=TRUE_SENTIMENT)) +
geom_point(alpha=0.8) +
ggtitle("Scatter plot of number of commas by each sentiment class") +
ylab("Number of Commas") +
xlab("Sentiment Class")
ggplotly(pl4)
sb_locs <- read_csv("starbucks-locations.csv", show_col_types=FALSE)
sb_nutr <- read_csv("starbucks-menu-nutrition.csv", show_col_types=FALSE)
usa_pop <- read_csv("us_state_pop.csv", show_col_types=FALSE)
usa_states<-read_csv("states.csv", show_col_types=FALSE)
sb_usa <- sb_locs |> filter(
# keep only those in the US
Country=="US"
)
sb_locs_state <- sb_usa |>
group_by(
# state
`State/Province`
) |>
rename(
# optional
state = `State/Province`
) |>
summarize(
# number of stores in each state
n_stores= n()
)
# need state abbreviations
usa_pop_abbr <-
full_join(
# 1. usa_pop
# 2. usa_states
usa_pop, usa_states, by = join_by(state == State)
)
sb_locs_state <- full_join(sb_locs_state, usa_pop_abbr, by = join_by(state==Abbreviation))
summary(sb_locs_state)
pl <- ggplot(sb_locs_state, aes(x=population, y=n_stores, color=state)) +
geom_point(alpha=0.8)
ggplotly(pl)
pl2 <- ggplot(sb_nutr, aes(x=Calories, fill=Category)) +
geom_histogram(alpha=0.5, position="identity") + theme_minimal()
ggplotly(pl2)
# TODO:
p3<- sb_nutr |>
unnest_tokens(word, Item, token="words") |>
count(word, sort=T) |>
head(20) |>
ggplot(aes(x=word, y=n))+
geom_col()+
coord_flip()
ggplotly(p3)
sb_nutr |>
plot_ly(x = ~Calories,
y = ~`Carb. (g)`,
type = "scatter",
mode = "markers",
color = ~Category)
df_unq <- read_csv("author_sentiment_unique.csv", show_col_types=FALSE)
pl5 <- ggplot(df_unq, aes(x=unique_word_count, fill=TRUE_SENTIMENT)) +
geom_histogram(alpha=0.5, position="identity") + theme_minimal()
ggplotly(pl5)
# Customize stopwords
stopwords2 <-c(stopwords("english"), "also", "using", "use", "used", "s", "t")
p3<- df |>
select(text_cleaned) |>
unnest_tokens(word, text_cleaned, token="words") |>
group_by(word) |>
summarise(word_frequency=n()) |>
arrange(across(word_frequency, desc)) |>
filter(!(word %in% stopwords2)) |>
filter(!(grepl("[[:digit:]]+", word))) |>
head(20) |>
ggplot(aes(reorder(word, -word_frequency), word_frequency)) +
geom_col()+
coord_flip() +
ggtitle("Top 20 common words in the document") +
ylab("Frequency of words") +
xlab("Words") + theme_minimal()
ggplotly(p3)
pl5 <- ggplot(df_unq, aes(x=unique_word_count, fill=TRUE_SENTIMENT)) +
geom_histogram(alpha=0.5, position="identity") + theme_minimal() +
ggtitle("Histogram of unique word count by sentiment class") +
ylab("Frequency") +
xlab("Count of Unique Words")
ggplotly(pl5)
pl5 <- ggplot(df_unq, aes(x=unique_word_count, fill=TRUE_SENTIMENT)) +
geom_histogram(alpha=0.5, position="identity") + theme_minimal() +
ggtitle("Histogram of unique word count by sentiment class") +
ylab("Frequency") +
xlab("Count of Unique Words")
ggplotly(pl5)
rmarkdown::render_site()
View(df_unq)
