---
title: "Interactive Visualization"
author: ""
output: 
    html_document:
        toc: TRUE
        toc_float: TRUE
---

```{r message=FALSE, echo=FALSE, warning=FALSE}
library(tidyverse)
library(plotly)
library(widgetframe)
library(tidytext)
library(wordcloud)
library(tm)
library(topicmodels)
library(NLP)
library(RColorBrewer)
```

```{r, echo=FALSE, warning=FALSE, include=FALSE}
# source("process_starbucks_data.R")
df <- read_csv("author_sentiment.csv", show_col_types=FALSE)
```

## 1. Word Count in the Textual Data
This plot describes the most common 20 words throughout the textual data after omitting stopwords. Notice few of the most common words are “Trump”, “new”, and “President”. This is potentially because many news articles retrieved for this dataset were written about the new U.S. president becoming Donald Trump, at the time. This depicts many entities in the dataset had topics regarding the new president, Donald Trump or U.S. politics.

```{r echo=FALSE}
# Customize stopwords
stopwords2 <-c(stopwords("english"), "also", "using", "use", "used", "s", "t")

p3<- df |>
  select(text_cleaned) |>
  unnest_tokens(word, text_cleaned, token="words") |>
  group_by(word) |>
  summarise(word_frequency=n()) |>
  arrange(across(word_frequency, desc)) |>
  filter(!(word %in% stopwords2)) |>
  filter(!(grepl("[[:digit:]]+", word))) |>
  head(20) |>
  ggplot(aes(reorder(word, -word_frequency), word_frequency)) +
  geom_col()+
  coord_flip() +
  ggtitle("Top 20 common words in the document") + 
  ylab("Frequency of words") + 
  xlab("Words") + theme_minimal()

ggplotly(p3)
```

## 2. Multiple Scatter Plots for Differnt Features {.tabset}
These are the scatter plots of some features retrieved from parts-of-speech tagging. Although there is not much difference in the average length of token for each sentiment class, for the number of commas, the neutral class has a relatively high number of commas compared to other classes. Further, the number of past tense verbs is sparse for the neutral class, while the positive and negative classes are dense, around 0 to 100 counts. Similarly, the number of proper nouns followed a similar trend: the neutral class is sparse, the negative class is dense around 0 to 200, and the positive class is dense around 0 to 300.

### Figure 1

```{r echo=FALSE}
pl <- ggplot(df, aes(x=TRUE_SENTIMENT, y=avg_len_token, color=TRUE_SENTIMENT)) + 
  geom_point(alpha=0.8) + 
  ggtitle("Scatter plot of average length of token by each sentiment class") + 
  ylab("Average Length of Token") + 
  xlab("Sentiment Class")
  
ggplotly(pl)
```

### Figure 2

```{r echo=FALSE}
pl2 <- ggplot(df, aes(x=TRUE_SENTIMENT, y=num_proper_noun, color=TRUE_SENTIMENT)) + 
  geom_point(alpha=0.8) + 
  ggtitle("Scatter plot of number of proper nouns by each sentiment class") + 
  ylab("Number of Proper Nouns") + 
  xlab("Sentiment Class")

ggplotly(pl2)
```
### Figure 3

```{r echo=FALSE}
pl3 <- ggplot(df, aes(x=TRUE_SENTIMENT, y=num_past_verb, color=TRUE_SENTIMENT)) + 
  geom_point(alpha=0.8) + 
  ggtitle("Scatter plot of number of past tense verbs by each sentiment class") + 
  ylab("Number of Past Tense Verbs") + 
  xlab("Sentiment Class")

ggplotly(pl3)
```

### Figure 4

```{r echo=FALSE}
pl4 <- ggplot(df, aes(x=TRUE_SENTIMENT, y=num_comma, color=TRUE_SENTIMENT)) + 
  geom_point(alpha=0.8) + 
  ggtitle("Scatter plot of number of commas by each sentiment class") + 
  ylab("Number of Commas") + 
  xlab("Sentiment Class")

ggplotly(pl4)
```

## 3. Number of Unique Words
This is a histogram of the unique word count by each sentiment class. Notice that the negative sentient class has a relatively low frequency of unique word count, whereas the neutral and positive classes have a relatively high frequency of unique word count. One of the reasons why the negative class has less distributed data is because it was over-sampled with replacement to account for the class imbalance in the original dataset. Although the class imbalance was fixed by over-sampling the negative sentiment class, this might have later caused the machine learning models to be more biased when making decisions regarding negative sentiment.

```{r, echo=FALSE, warning=FALSE, include=FALSE}
df_unq <- read_csv("author_sentiment_unique.csv", show_col_types=FALSE)
```

```{r, echo=FALSE, warning=FALSE, message=FALSE}
pl5 <- ggplot(df_unq, aes(x=unique_word_count, fill=TRUE_SENTIMENT)) + 
  geom_histogram(alpha=0.5, position="identity") + theme_minimal() + 
  ggtitle("Histogram of unique word count by sentiment class") + 
  ylab("Frequency") + 
  xlab("Count of Unique Words")

ggplotly(pl5)
```
